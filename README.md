# Twitter Vaccine Classificator

## Overview

Twitter Vaccine Classification is a project aimed at classifying tweets related to vaccines. 
It utilizes natural language processing (NLP) techniques and machine learning to categorize tweets into relevant categories.

## Features

- **Tweet Classification:** The system classifies tweets into different categories related to vaccines, such as positive, negative, or neutral sentiments.
- **Pretrained Model:** The project uses a particular version of BERT found in [https://github.com/marcopoli/AlBERTo-it], it's a pre-trained BERT already trained on Italian tweets, so it's efficient on the classification.
- **Network Analysis:** It is also possible to perform network analysis alongside NLP analysis. This involves constructing a graph and dividing it into communities. Additionally, users can be placed in a user-space based on their positions using the ForcaAtlas2 algorithm.
<div style="display: flex; justify-content: center;">
    <img src="https://github.com/Niconiki99/TwitterVaccineClassificator/raw/main/images/net_ld.png" alt="Leiden Network Visualization" width="500"/>
    <img src="https://github.com/Niconiki99/TwitterVaccineClassificator/raw/main/images/net_lv.png" alt="Louvain Network Visualization" width="500"/>
</div>
- **Multitabular Analysis:** To significantly increase the accuracy of our classification, this project is based on a machine learning method that enables the combination of NLP embeddings generated by BERT with other features, whether they are categorical or numerical. These features are based on network-related data, such as the user's community and position in the user space. Utilizing the powerful [https://github.com/georgian-io/Multimodal-Toolkit] library, we have implemented various methods to concatenate the NLP embeddings with the non-NLP features.
## Installation

1. Clone the repository:

    ```bash
    git clone https://github.com/Niconiki99/TwitterVaccineClassificator.git
    cd TwitterVaccineClassificator
    ```

2. Install dependencies:
Verify the dependencies on requirements.txt and install what is missing.

## Usage
The project is made up of several modular components, each designed to perform specific analyses independently. To conduct a comprehensive analysis, it is recommended to start with the build_graphs module. This step begins with raw tweets and selects only those that fall within a specific deadline. The module then constructs the graph, laying the foundation for subsequent analyses.

After graph construction, the build_communities module is used to examine the community structures within the graph. This step is critical in understanding the network's intrinsic groupings and relationships. Different algorithms can be used to build the communities.

The network.py module is responsible for computing the positions of user IDs within the graph. It also produces a visual representation of the network, assigning distinct colors to each community. This visualization helps in interpreting the network's structural dynamics and identifying the relationships between different user communities. Together with the information of the communities, the positions are the key features to couple with the embeddings performed by BERT to classify the tweets.

The next stage involves preprocessing tasks such as dataset merging, splitting, and other preparatory steps. The Preprocessing module manages this crucial phase, ensuring that the data is appropriately formatted and ready for subsequent analyses. This module couples all the features produced by the previous modules with the raw tweets.

Finally, the project includes the MultiBERT_train module, which focuses on training models based on the preprocessed data. This step allows for the development and refinement of models that can provide insights and predictions based on the characteristics of the network and user communities.
## Configuration
